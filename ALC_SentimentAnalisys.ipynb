{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vectorizer - ALC - SentimentAnalisys",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poketmonster/sentimentanalysis/blob/master/ALC_SentimentAnalisys.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "WCjRqsoOfJW2",
        "colab_type": "code",
        "outputId": "fe7a0854-6d05-4ebe-88b2-1ec72993eac9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "import csv, operator\n",
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "import math\n",
        "import random\n",
        "import xml.etree.ElementTree as et\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from random import shuffle\n",
        "from nltk.stem.porter import *\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_extraction import DictVectorizer"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "n8pU2F3YwWRX",
        "colab_type": "code",
        "outputId": "5da28fa4-0daf-42c8-b8be-0b2816654322",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "!rm -r *\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload() # then browse, select the files. It's then uploaded"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-16a84000-ca61-411c-84c0-be1e390c457a\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-16a84000-ca61-411c-84c0-be1e390c457a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving TASS2017_T1_development.xml to TASS2017_T1_development.xml\n",
            "Saving TASS2017_T1_test.xml to TASS2017_T1_test.xml\n",
            "Saving TASS2017_T1_training.xml to TASS2017_T1_training.xml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PF_4dNahG7Vp",
        "colab_type": "code",
        "outputId": "015b9359-0746-4498-c767-18d798810631",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "cell_type": "code",
      "source": [
        "!rm -r *\n",
        "!wget https://raw.githubusercontent.com/poketmonster/sentimentanalysis/master/TASS2017_T1_development.xml\n",
        "!wget https://raw.githubusercontent.com/poketmonster/sentimentanalysis/master/TASS2017_T1_test.xml\n",
        "!wget https://raw.githubusercontent.com/poketmonster/sentimentanalysis/master/TASS2017_T1_training.xml\n",
        "!wget https://raw.githubusercontent.com/poketmonster/sentimentanalysis/master/ElhPolar_esV1.lex"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-04-14 08:17:30--  https://raw.githubusercontent.com/poketmonster/sentimentanalysis/master/TASS2017_T1_development.xml\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 163475 (160K) [text/plain]\n",
            "Saving to: ‘TASS2017_T1_development.xml’\n",
            "\n",
            "\r          TASS2017_   0%[                    ]       0  --.-KB/s               \rTASS2017_T1_develop 100%[===================>] 159.64K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2019-04-14 08:17:30 (5.15 MB/s) - ‘TASS2017_T1_development.xml’ saved [163475/163475]\n",
            "\n",
            "--2019-04-14 08:17:32--  https://raw.githubusercontent.com/poketmonster/sentimentanalysis/master/TASS2017_T1_test.xml\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 610864 (597K) [text/plain]\n",
            "Saving to: ‘TASS2017_T1_test.xml’\n",
            "\n",
            "TASS2017_T1_test.xm 100%[===================>] 596.55K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2019-04-14 08:17:32 (11.4 MB/s) - ‘TASS2017_T1_test.xml’ saved [610864/610864]\n",
            "\n",
            "--2019-04-14 08:17:33--  https://raw.githubusercontent.com/poketmonster/sentimentanalysis/master/TASS2017_T1_training.xml\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 325931 (318K) [text/plain]\n",
            "Saving to: ‘TASS2017_T1_training.xml’\n",
            "\n",
            "TASS2017_T1_trainin 100%[===================>] 318.29K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2019-04-14 08:17:33 (7.74 MB/s) - ‘TASS2017_T1_training.xml’ saved [325931/325931]\n",
            "\n",
            "--2019-04-14 08:17:35--  https://raw.githubusercontent.com/poketmonster/sentimentanalysis/master/ElhPolar_esV1.lex\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 99625 (97K) [text/plain]\n",
            "Saving to: ‘ElhPolar_esV1.lex’\n",
            "\n",
            "ElhPolar_esV1.lex   100%[===================>]  97.29K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2019-04-14 08:17:35 (3.14 MB/s) - ‘ElhPolar_esV1.lex’ saved [99625/99625]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Nbpmp5rhZZmu",
        "colab_type": "code",
        "outputId": "1adbcad6-2da3-4f2e-9fb4-7ecb2c4d9864",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "#Cargar diccionario de polaridad\n",
        "\n",
        "v = DictVectorizer(sparse=False)\n",
        "s = open('ElhPolar_esV1.lex', 'r').read()\n",
        "\n",
        "\n",
        "palabra = []\n",
        "polaridad = []\n",
        "with open('ElhPolar_esV1.lex') as f: \n",
        "  for line in f.readlines():\n",
        "    div = line.split('\\t')\n",
        "    if len(div)==2:\n",
        "      palabra.append(div[0])\n",
        "      polaridad.append(div[1].replace(\"\\n\", \"\"))\n",
        "  \n",
        "Poldict = {}\n",
        "for i in range(len(palabra)):\n",
        "    Poldict[palabra[i]] = polaridad[i]\n",
        "    \n",
        "print(len(Poldict))\n",
        "Poldict['abdicar']"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4673\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'negative'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "1V8XhMiDhhce",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def parse_XML(xml_file, df_cols): \n",
        "    \"\"\"Parse the input XML file and store the result in a pandas DataFrame \n",
        "    with the given columns. The first element of df_cols is supposed to be \n",
        "    the identifier variable, which is an attribute of each node element in \n",
        "    the XML data; other features will be parsed from the text content of \n",
        "    each sub-element. \"\"\"\n",
        "    \n",
        "    xtree = et.parse(xml_file)\n",
        "    xroot = xtree.getroot()\n",
        "    out_df = pd.DataFrame(columns = df_cols)\n",
        "    \n",
        "    for node in xroot: \n",
        "        res = []\n",
        "        for el in df_cols[0:]: \n",
        "            if node is not None and node.find(el) is not None:\n",
        "                res.append(node.find(el).text)\n",
        "            else: \n",
        "                res.append(None)\n",
        "        out_df = out_df.append(pd.Series(res, index = df_cols), ignore_index = True)\n",
        "        \n",
        "    return out_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MjGt4u2lfKj0",
        "colab_type": "code",
        "outputId": "572aedeb-a036-449e-a564-706c2f8d773c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "cell_type": "code",
      "source": [
        "train = parse_XML(\"TASS2017_T1_training.xml\", [\"tweetid\", \"user\", \"content\", \"date\", \"lang\", \"./sentiment/polarity/value\"])\n",
        "print(\"TRAIN\")\n",
        "train.columns = [\"tweetid\", \"user\", \"content\", \"date\", \"lang\", \"polarity\"]\n",
        "#print(train.head())\n",
        "print(\"Langs: \"+str(train.lang.unique()))\n",
        "print(train['polarity'].value_counts(dropna=False))\n",
        "\n",
        "dev = parse_XML(\"TASS2017_T1_development.xml\", [\"tweetid\", \"user\", \"content\", \"date\", \"lang\", \"./sentiment/polarity/value\"])\n",
        "print(\"DEV\")\n",
        "dev.columns = [\"tweetid\", \"user\", \"content\", \"date\", \"lang\", \"polarity\"]\n",
        "#print(dev.head())\n",
        "print(\"Langs: \"+str(dev.lang.unique()))\n",
        "print(dev['polarity'].value_counts(dropna=False))\n",
        "\n",
        "test = parse_XML(\"TASS2017_T1_test.xml\", [\"tweetid\", \"user\", \"content\", \"date\", \"lang\", \"./sentiment/polarity/value\"])\n",
        "print(\"TEST\")\n",
        "test.columns = [\"tweetid\", \"user\", \"content\", \"date\", \"lang\", \"polarity\"]\n",
        "#print(test.head())\n",
        "print(\"Langs: \"+str(test.lang.unique()))\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN\n",
            "Langs: ['es']\n",
            "N       418\n",
            "P       318\n",
            "NONE    139\n",
            "NEU     133\n",
            "Name: polarity, dtype: int64\n",
            "DEV\n",
            "Langs: ['es']\n",
            "N       219\n",
            "P       156\n",
            "NEU      69\n",
            "NONE     62\n",
            "Name: polarity, dtype: int64\n",
            "TEST\n",
            "Langs: ['es']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "u5JRHqUqY9g4",
        "colab_type": "code",
        "outputId": "745fb20c-9d91-4636-e4d3-1a362a5ed1f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "tknzr = TweetTokenizer()\n",
        "\n",
        "trainCorpus = []\n",
        "train_labels = []\n",
        "\n",
        "devCorpus = []\n",
        "dev_labels = []\n",
        "\n",
        "for i in range(0,len(train)):\n",
        "  senseurl = re.sub(r\"http\\S+\", \"\", train.at[i, 'content'])\n",
        "  tokenitzat = tknzr.tokenize(senseurl)\n",
        "  trainCorpus.append(train.at[i, 'content'])\n",
        "  train_labels.append(train.at[i,'polarity'])\n",
        "\n",
        "for i in range(0,len(dev)):\n",
        "  senseurl = re.sub(r\"http\\S+\", \"\", dev.at[i, 'content'])\n",
        "  tokenitzat = tknzr.tokenize(senseurl)\n",
        "  devCorpus.append(dev.at[i, 'content'])\n",
        "  dev_labels.append(dev.at[i,'polarity'])\n",
        "  \n",
        "print(len(trainCorpus))\n",
        "print(len(train_labels))\n",
        "print(len(devCorpus))\n",
        "print(len(dev_labels))\n",
        "\n",
        "#devCorpus"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1008\n",
            "1008\n",
            "506\n",
            "506\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BVuyYXUACbl4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from nltk.data import load\n",
        "from nltk.stem import SnowballStemmer\n",
        "from string import punctuation\n",
        "\n",
        "spanish_stopwords = stopwords.words('spanish')\n",
        "\n",
        "stemmer = SnowballStemmer('spanish')\n",
        "\n",
        "non_words = list(punctuation)\n",
        "non_words.extend(['¿', '¡'])\n",
        "non_words.extend(map(str,range(10)))\n",
        "\n",
        "stemmer = SnowballStemmer('spanish')\n",
        "def stem_tokens(tokens, stemmer):\n",
        "    stemmed = []\n",
        "    for item in tokens:\n",
        "        stemmed.append(stemmer.stem(item))\n",
        "    return stemmed\n",
        "\n",
        "\n",
        "def onwtokenizer(text):\n",
        "  text = re.sub(r\"http\\S+\", \"\", text)\n",
        "  text = ''.join([c for c in text if c not in non_words])\n",
        "  tokens =  word_tokenize(text)\n",
        "\n",
        "  # stem\n",
        "  try:\n",
        "      stems = stem_tokens(tokens, stemmer)\n",
        "  except Exception as e:\n",
        "      print(e)\n",
        "      print(text)\n",
        "      stems = ['']\n",
        "  return stems\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "kBDqo8WKipf7",
        "outputId": "ec7feaa8-a562-4aea-fdb6-63518dbb9eaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        }
      },
      "cell_type": "code",
      "source": [
        "# TfidfVectorizer CHAR\n",
        "vectorizer = TfidfVectorizer(min_df=1,\n",
        "                             max_df = 0.9,\n",
        "                             sublinear_tf=True,\n",
        "                             analyzer='char_wb',\n",
        "                             ngram_range = (1,20),\n",
        "                             max_features = None,\n",
        "                             strip_accents = \"unicode\",\n",
        "                             use_idf=True)\n",
        "\n",
        "\n",
        "train_vectors = vectorizer.fit_transform(trainCorpus)\n",
        "dev_vectors = vectorizer.transform(devCorpus)\n",
        "\n",
        "\n",
        "print(train_vectors[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 90759)\t0.021646297666471075\n",
            "  (0, 92073)\t0.024171193599014328\n",
            "  (0, 61809)\t0.015282168306276525\n",
            "  (0, 92074)\t0.01833129221077423\n",
            "  (0, 53208)\t0.015528592946580664\n",
            "  (0, 68581)\t0.017481383547965138\n",
            "  (0, 135451)\t0.011127238062587542\n",
            "  (0, 19244)\t0.019407578324254528\n",
            "  (0, 51194)\t0.02010850370845873\n",
            "  (0, 11066)\t0.01951106173362051\n",
            "  (0, 65583)\t0.010649412351979066\n",
            "  (0, 110962)\t0.020847711534352453\n",
            "  (0, 23038)\t0.02285419671886908\n",
            "  (0, 111022)\t0.02119753926552142\n",
            "  (0, 129690)\t0.01829536300057581\n",
            "  (0, 23050)\t0.023288304611244502\n",
            "  (0, 111023)\t0.022279155140902653\n",
            "  (0, 129691)\t0.013673438930637084\n",
            "  (0, 23051)\t0.02388005162807004\n",
            "  (0, 111024)\t0.01381985119016975\n",
            "  (0, 23052)\t0.014649503252231648\n",
            "  (0, 83590)\t0.03944507598607009\n",
            "  (0, 72747)\t0.011341098106817767\n",
            "  (0, 18283)\t0.025222173475965823\n",
            "  (0, 47883)\t0.01289089510493581\n",
            "  :\t:\n",
            "  (0, 18284)\t0.03268624479032971\n",
            "  (0, 83695)\t0.0727087826919365\n",
            "  (0, 44118)\t0.06753564578811726\n",
            "  (0, 18295)\t0.03510313535604631\n",
            "  (0, 83699)\t0.0727087826919365\n",
            "  (0, 44139)\t0.061129180701801615\n",
            "  (0, 44119)\t0.036946290208092696\n",
            "  (0, 18296)\t0.0353418251035917\n",
            "  (0, 83713)\t0.061129180701801615\n",
            "  (0, 44143)\t0.062063177210225\n",
            "  (0, 83700)\t0.037561633193896396\n",
            "  (0, 18302)\t0.036946290208092696\n",
            "  (0, 83717)\t0.06255550709798166\n",
            "  (0, 44154)\t0.03895567792415006\n",
            "  (0, 44144)\t0.03895567792415006\n",
            "  (0, 18304)\t0.03788802597164124\n",
            "  (0, 83728)\t0.03934578030393423\n",
            "  (0, 44158)\t0.04064520247715942\n",
            "  (0, 83718)\t0.03934578030393423\n",
            "  (0, 18306)\t0.04018819165042352\n",
            "  (0, 83732)\t0.04064520247715942\n",
            "  (0, 44159)\t0.04279011060285369\n",
            "  (0, 18309)\t0.04112992741397206\n",
            "  (0, 83733)\t0.04279011060285369\n",
            "  (0, 18310)\t0.047739646487594\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bOimToV73PmW",
        "colab_type": "code",
        "outputId": "6ea893b5-1921-4049-8d98-c440d283b193",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "cell_type": "code",
      "source": [
        "# TfidfVectorizer WORDS\n",
        "vectorizer = TfidfVectorizer(min_df=1,\n",
        "                             max_df = 0.99,\n",
        "                             tokenizer = onwtokenizer,\n",
        "                             sublinear_tf=False,\n",
        "                             analyzer='word',\n",
        "                             ngram_range = (1,1),\n",
        "                             max_features = None,\n",
        "                             strip_accents = \"ascii\",\n",
        "                             lowercase = True,\n",
        "                             smooth_idf = True,\n",
        "                             use_idf=True)\n",
        "\n",
        "train_vectors = vectorizer.fit_transform(trainCorpus)\n",
        "dev_vectors = vectorizer.transform(devCorpus)\n",
        "\n",
        "print(vectorizer.get_feature_names())\n",
        "print(train_vectors.shape)\n",
        "\n",
        "print(train_vectors[1])"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['a', 'aaahm', 'aaronnp', 'abac', 'abandon', 'abat', 'abces', 'aberron', 'abiert', 'abraz', 'abrazar', 'abrazart', 'abrazotee', 'abre', 'abres', 'abrig', 'abrilnapilotti', 'abrir', 'absolut', 'abstract', 'absurd', 'abuel', 'aburr', 'ac', 'acab', 'acapulcoshor', 'acc', 'acced', 'accion', 'ace', 'acept', 'acerc', 'acert', 'acevasociacion', 'achocoblog', 'achuch', 'aciert', 'acompan', 'acondicion', 'acongoj', 'acord', 'acos', 'acostumbr', 'activ', 'actor', 'actriz', 'actuacion', 'actualiz', 'acuerd', 'acust', 'adapt', 'adaptacion', 'adc', 'adem', 'adiccion', 'adioshi', 'adirect', 'adjet', 'adolescent', 'adopt', 'ador', 'adriansol', 'adribuz', 'adrimarinab', 'adsu', 'advert', 'advirti', 'afeit', 'afon', 'afor', 'afortun', 'agarotadipanem', 'agarzon', 'agenci', 'agmelendez', 'agnesfl', 'agobi', 'agost', 'agot', 'agradec', 'agreg', 'agresion', 'agresor', 'agu', 'aguant', 'ah', 'ahi', 'ahilodej', 'ahor', 'ahorapod', 'ahorr', 'ahurtadobuen', 'ai', 'aikoleru', 'ainhoamall', 'air', 'airatarr', 'aisla', 'aitor', 'aiud', 'ajajaj', 'ajaxajaxcf', 'ajpaniagu', 'akti', 'al', 'ala', 'aladroq', 'alarm', 'alastrain', 'alba', 'albabenit', 'albeferr', 'albert', 'albertriver', 'albgard', 'albuet', 'alcald', 'alcanc', 'alcohol', 'alegr', 'alegri', 'alelu', 'alexbellamylet', 'alexdi', 'alexi', 'alexyazaw', 'algecir', 'algo', 'algoestacambi', 'algui', 'algun', 'ali', 'alici', 'aliciagonzg', 'alienmar', 'alli', 'almeriens', 'aloj', 'alquil', 'alta', 'altafitdonosti', 'altealasuec', 'altern', 'alto', 'alvaroauryn', 'am', 'amabl', 'amaiah', 'amarg', 'amartin', 'amazon', 'ambient', 'amen', 'ami', 'amig', 'amiks', 'amist', 'amo', 'amor', 'ampli', 'amuntvalent', 'ana', 'anad', 'anadarm', 'anamil', 'anayainfantil', 'ancarlol', 'ancha', 'andam', 'andar', 'andarellabcn', 'anden', 'ando', 'andre', 'andreanaved', 'andres', 'angelam', 'angelesuc', 'angelet', 'angstiness', 'angul', 'angybir', 'anhel', 'anijajajaj', 'anim', 'animal', 'anniebooksx', 'annlloret', 'ano', 'anoch', 'anordest', 'anos', 'anterior', 'antes', 'antibiot', 'antif', 'antigu', 'antimadridvcf', 'antobosc', 'antoj', 'antonioortzb', 'antoniopv', 'antoniosant', 'antonioviol', 'antonlof', 'anunci', 'aodimi', 'apag', 'apan', 'aparec', 'apart', 'apen', 'api', 'aplaud', 'aport', 'aportacion', 'apoyocr', 'app', 'apreci', 'aprend', 'aprob', 'aprovech', 'aproximacion', 'aprueb', 'apuf', 'apunt', 'aqi', 'aquel', 'aquell', 'aqueltien', 'aqui', 'aquilatiki', 'arab', 'arbitrari', 'arcoiris', 'ares', 'argeli', 'ariasemmap', 'ariver', 'arma', 'armas', 'armoni', 'arregl', 'arrib', 'arriend', 'arruin', 'articul', 'artificialr', 'artist', 'arvenlol', 'as', 'asegur', 'asi', 'asidesastr', 'asient', 'asimetr', 'asimil', 'asistent', 'asit', 'ask', 'askalvarogang', 'asklfi', 'asociacion', 'aspect', 'aspir', 'astral', 'asukawach', 'asunt', 'asust', 'asustadiz', 'atac', 'atardec', 'atend', 'atent', 'atras', 'atrev', 'audi', 'audreymolg', 'aun', 'aunqu', 'autoestimaeg', 'autoexploracion', 'autop', 'autor', 'autorizacion', 'ave', 'aventur', 'avionet', 'avis', 'award', 'ax', 'ay', 'ayay', 'ayer', 'ayud', 'ayudam', 'ayudaslfi', 'ayuntamientoibi', 'ayy', 'b', 'bab', 'babidark', 'bacala', 'bacteri', 'bad', 'bae', 'bail', 'bailart', 'baj', 'bajon', 'bal', 'balist', 'ban', 'banc', 'bar', 'barat', 'barc', 'barcelon', 'baronachor', 'barrig', 'barriobajer', 'barrosnere', 'bas', 'bastant', 'basur', 'bat', 'batall', 'battlefield', 'batyol', 'bautist', 'bayonth', 'bbs', 'bc', 'bcn', 'be', 'beatriceval', 'beatriizzzz', 'beautyblog', 'beb', 'becari', 'beki', 'bel', 'belenfgonzalez', 'bell', 'belunitaa', 'bendicion', 'benidorm', 'benielanaisabel', 'bensonsenor', 'beret', 'bernari', 'berni', 'berothw', 'bes', 'besit', 'bet', 'betemili', 'betisfans', 'betterton', 'betvs', 'betwin', 'beyondhell', 'bhmallorc', 'bhmallorcamagaluf', 'bib', 'bibliotec', 'bibliotecaibi', 'bici', 'bien', 'bigzetayt', 'bikini', 'bilba', 'billet', 'biolog', 'bird', 'blasauryn', 'blastois', 'blme', 'blog', 'blogs', 'bloqu', 'blssur', 'bnxes', 'boardgametotal', 'boc', 'bocadient', 'bocadill', 'bod', 'boll', 'boller', 'bonic', 'bonit', 'bonitoy', 'bonus', 'borr', 'boski', 'boterobet', 'boton', 'botoxil', 'bradinh', 'brasilen', 'brev', 'brey', 'britney', 'bro', 'brom', 'broomkurosaw', 'brujadeltuiters', 'brujulaondacer', 'bryan', 'bryantorresfsh', 'bucl', 'buen', 'buenisim', 'buenisism', 'buenosdi', 'bullying', 'burgu', 'burkini', 'burocrat', 'burr', 'bus', 'busc', 'buscam', 'buscandot', 'buscar', 'butfilmfestival', 'bvillamorgay', 'bystaxx', 'c', 'cab', 'cabez', 'cabl', 'cabron', 'cachond', 'cachopin', 'cad', 'caden', 'cadist', 'cae', 'caem', 'caen', 'caer', 'caes', 'cafelluvi', 'cag', 'cagadisim', 'caid', 'caig', 'calcetin', 'calendari', 'calent', 'calid', 'call', 'calla', 'calladit', 'calor', 'calv', 'calvin', 'cam', 'cambi', 'came', 'camilleblond', 'camin', 'camis', 'camiset', 'campan', 'camr', 'can', 'canal', 'canalfiest', 'cancel', 'cancion', 'candiecit', 'canel', 'cangasdeonis', 'cans', 'cant', 'cantabriamiguel', 'cantagalli', 'capaz', 'capci', 'capit', 'capitul', 'caps', 'captur', 'capull', 'car', 'caracter', 'caramusg', 'caratul', 'carcel', 'caret', 'carg', 'cari', 'carin', 'carisimolid', 'carit', 'carl', 'carlesrh', 'carlosbardem', 'carlosmchef', 'carlosmorenoch', 'carlossadness', 'carlotalotalot', 'carlotayoutub', 'carmenagustin', 'carmesi', 'carn', 'carnecrudaradi', 'carolinaftam', 'carrasc', 'carrefour', 'carrer', 'carreter', 'carrill', 'cart', 'cas', 'casari', 'casc', 'cascarel', 'casi', 'casit', 'caspito', 'casters', 'casual', 'cat', 'catalog', 'caus', 'cayol', 'cayusoll', 'caz', 'cca', 'ccc', 'cdecienci', 'cdlared', 'cduranestany', 'cej', 'celebr', 'celiaogall', 'celopanyt', 'cen', 'ceniz', 'cens', 'centaur', 'centauri', 'cer', 'cerc', 'cerd', 'cereal', 'cerny', 'cerquit', 'cervez', 'ces', 'cesarcpi', 'cesartgil', 'chachis', 'champions', 'chanquetefest', 'chaquet', 'charl', 'charliereckless', 'charmimedic', 'chatel', 'che', 'chic', 'chica', 'chicacascabel', 'chich', 'chicxs', 'chin', 'chiqui', 'chiquitin', 'chiringuitopiqu', 'chiscoy', 'chist', 'chistorr', 'choc', 'chooficial', 'chorraer', 'chuec', 'chul', 'chup', 'chut', 'cicatric', 'cieg', 'ciel', 'cien', 'cientif', 'cierr', 'ciert', 'ciertom', 'cigotenks', 'cin', 'cinc', 'cinedepati', 'cinefil', 'circul', 'circunst', 'cit', 'citaconsilvi', 'citiz', 'ciud', 'ciudadan', 'ciudadani', 'ciudadano', 'ciudadanoscs', 'clar', 'claranyanny', 'clarit', 'clarooo', 'clas', 'clasic', 'clasif', 'clasificacion', 'claudiapiquer', 'claunavarr', 'clex', 'clic', 'click', 'clientehyundai', 'clip', 'clrealy', 'club', 'cobard', 'cobertur', 'cobosrebec', 'cobr', 'coc', 'coch', 'cocinadeenloqui', 'cod', 'cog', 'coj', 'cojon', 'cojonud', 'col', 'colabor', 'colaboracion', 'colaca', 'colect', 'colegi', 'colg', 'color', 'com', 'comb', 'combat', 'comedi', 'comenz', 'comer', 'comet', 'cometel', 'comic', 'comics', 'comienc', 'comienz', 'comill', 'comisari', 'companer', 'compart', 'compartais', 'compat', 'compit', 'complac', 'complej', 'complement', 'complet', 'completit', 'complic', 'complicadill', 'compr', 'comprart', 'comprat', 'comprend', 'comprendais', 'comprens', 'comun', 'comunidadff', 'con', 'concejal', 'concept', 'conch', 'concht', 'conciert', 'concurs', 'condicion', 'conect', 'confi', 'confirm', 'confirma', 'conformat', 'confund', 'congres', 'conmig', 'conmigooooooo', 'conmtvaltour', 'conoc', 'conocert', 'conoci', 'conozc', 'conpdejulandron', 'conscienci', 'consej', 'conserv', 'consider', 'consig', 'constanci', 'constellation', 'constru', 'construi', 'consuecaball', 'consult', 'consultarmel', 'consumidor', 'cont', 'contact', 'contagi', 'contar', 'content', 'contest', 'contig', 'continu', 'contr', 'contrari', 'contrat', 'contratacion', 'control', 'convenc', 'convenci', 'convenient', 'convenz', 'convers', 'conversacion', 'convocatori', 'cop', 'copi', 'corasonsit', 'corazon', 'corean', 'corpor', 'corr', 'corre', 'correccion', 'correct', 'correg', 'corrient', 'corrupcionpp', 'cors', 'cort', 'cortit', 'corun', 'cos', 'cosit', 'cosplay', 'cost', 'cotidian', 'craci', 'crack', 'cre', 'crec', 'credit', 'creep', 'crei', 'crem', 'crest', 'cretac', 'criadoenter', 'crimsonghost', 'cris', 'crisasanz', 'crisdazg', 'crisis', 'crislalid', 'crisnosesa', 'cristal', 'cristin', 'criteri', 'critic', 'cronic', 'crowfunding', 'crtkftauryn', 'cruel', 'crush', 'crususmusus', 'cruyss', 'cruz', 'cuadr', 'cual', 'cualqu', 'cualqui', 'cuand', 'cuandoabaj', 'cuant', 'cuart', 'cub', 'cubr', 'cuch', 'cuel', 'cuent', 'cuerd', 'cuerp', 'cuestion', 'cuid', 'cul', 'culp', 'cultrun', 'cultur', 'culturplaz', 'cumpl', 'cumplean', 'cumpli', 'cuqui', 'cur', 'curios', 'curr', 'curric', 'currin', 'curs', 'cuteres', 'cuticul', 'cutr', 'cyborug', 'cydleones', 'd', 'da', 'dab', 'dad', 'dadl', 'dadm', 'dafnedelray', 'dal', 'dalekf', 'dan', 'danc', 'dancing', 'dand', 'dani', 'danigarciac', 'danirs', 'danixelcox', 'dantach', 'dar', 'darkfabi', 'darkportu', 'darl', 'darm', 'dars', 'darthmagnus', 'dat', 'dav', 'davidarizaab', 'davidcpsberg', 'davidg', 'davidmaeztu', 'day', 'dcodefest', 'ddh', 'de', 'death', 'deb', 'debat', 'deber', 'deberi', 'debi', 'debil', 'dec', 'decant', 'decent', 'decepcion', 'decid', 'decidl', 'decirt', 'ded', 'dedic', 'defens', 'defensor', 'defib', 'defin', 'definicion', 'deibiz', 'dej', 'dejadm', 'dejar', 'dejart', 'del', 'delant', 'delat', 'delg', 'delici', 'demand', 'demasi', 'democrat', 'demoni', 'demoslelavuelt', 'demostr', 'den', 'denebgab', 'denisfmendez', 'dentr', 'denunci', 'dep', 'depresion', 'der', 'derech', 'deret', 'derrit', 'des', 'desagrad', 'desaparec', 'descalz', 'descans', 'descar', 'descarg', 'descart', 'descojon', 'descomunal', 'desconoc', 'descubr', 'descuent', 'desd', 'dese', 'desesperanz', 'desgraci', 'desiert', 'designcampfest', 'desmoviliz', 'desnud', 'desped', 'despeg', 'despert', 'despiert', 'despu', 'destempl', 'destin', 'desventur', 'desvi', 'desviv', 'detall', 'dexamin', 'dgtes', 'dia', 'diamond', 'dianamarqus', 'dias', 'dibuj', 'dic', 'dich', 'diegofdm', 'diegopaj', 'dierap', 'diet', 'diferent', 'dificil', 'dig', 'digard', 'dij', 'dilem', 'dim', 'dimequefuedeti', 'din', 'diner', 'dio', 'dios', 'diput', 'dir', 'direct', 'directill', 'diri', 'discurs', 'disen', 'disfrut', 'dispon', 'disposit', 'dispuest', 'distincion', 'distint', 'distr', 'divert', 'divid', 'divin', 'division', 'divorci', 'divulg', 'dm', 'dnd', 'dni', 'dobl', 'doc', 'doctorgaon', 'dol', 'dolor', 'doloresparis', 'doming', 'dominguezj', 'don', 'dond', 'donecperficiam', 'doni', 'donosti', 'donostiarr', 'dorm', 'dormi', 'dos', 'dosl', 'dovesandletters', 'down', 'doy', 'dragonit', 'dream', 'drown', 'drxaverius', 'ds', 'dtmspain', 'dud', 'duel', 'duerm', 'dukeapologi', 'dur', 'durais', 'durant', 'e', 'eajpnv', 'echa', 'echad', 'echan', 'echand', 'echarl', 'echarm', 'echat', 'eche', 'echo', 'eco', 'econom', 'ecosistem', 'ed', 'edad', 'edgarcabrerag', 'edicion', 'edifici', 'edortaramirez', 'edtve', 'eduardopers', 'educ', 'educacion', 'eduemoesc', 'edugal', 'edureus', 'edurniev', 'efect', 'egoist', 'eh', 'ej', 'ejem', 'ejempl', 'el', 'elapersw', 'eldesmarquevcf', 'eled', 'elespanolcom', 'eliperezup', 'ella', 'ellas', 'elli', 'ello', 'ellos', 'elmund', 'elo', 'elpachink', 'elpais', 'elpaisespan', 'elprogramad', 'elpueblomaldit', 'elrabodech', 'elspeak', 'eltuitdrink', 'elvirach', 'email', 'embarc', 'emberd', 'emimartinez', 'emision', 'emit', 'emo', 'emocion', 'emocional', 'emot', 'emoticon', 'empati', 'empez', 'empiec', 'empiez', 'emple', 'emprendedor', 'empres', 'emul', 'en', 'enamor', 'enan', 'encant', 'encantari', 'encarg', 'encim', 'encontr', 'encuentr', 'encueshshtasshs', 'enekohumor', 'enemig', 'enfad', 'enfermeri', 'enfrent', 'engan', 'enganch', 'enhorabuen', 'enlacabezan', 'enmarc', 'enmare', 'enmascar', 'enquist', 'ensal', 'ensen', 'enseri', 'entend', 'enter', 'enternecedor', 'entiend', 'entonc', 'entra', 'entrab', 'entrand', 'entrar', 'entrarm', 'entras', 'entre', 'entren', 'entreten', 'entrev', 'entrevist', 'entristec', 'entro', 'entuert', 'envi', 'envidi', 'envidiosill', 'episodi', 'equip', 'equivoc', 'era', 'eran', 'eras', 'erchach', 'eres', 'ericelhuman', 'erlik', 'es', 'esa', 'esas', 'escamp', 'escan', 'escap', 'escapeofthecrow', 'escaqu', 'escarmentams', 'escenari', 'escog', 'escol', 'escolan', 'escot', 'escrib', 'escrit', 'escritor', 'escuarantul', 'escuch', 'escuchal', 'escucheis', 'ese', 'esfuerz', 'eslasentent', 'eslosiguient', 'eso', 'esos', 'espaci', 'espald', 'espan', 'espanol', 'espant', 'espanyol', 'esparv', 'especial', 'espectacul', 'especulacioncp', 'espej', 'esper', 'espum', 'esqu', 'esquin', 'esquinit', 'esroy', 'essiesenn', 'essucultur', 'esta', 'estab', 'establ', 'estad', 'estais', 'estam', 'estan', 'estar', 'estari', 'estas', 'estat', 'estatal', 'este', 'esteb', 'esten', 'estet', 'estherct', 'estherswaggy', 'estil', 'estir', 'esto', 'estop', 'estornud', 'estos', 'estoy', 'estratific', 'estrell', 'estrellalev', 'estren', 'estrict', 'estrop', 'estructur', 'estudi', 'estupend', 'estuv', 'etap', 'etc', 'etern', 'eur', 'europ', 'europapress', 'eutrofiz', 'evagelcot', 'evan', 'evans', 'evident', 'evit', 'evolucion', 'ex', 'exam', 'examen', 'exist', 'exit', 'expert', 'explic', 'explor', 'explot', 'expo', 'exposicion', 'expulsion', 'exquisit', 'extermin', 'extran', 'extraordinerd', 'extrem', 'extudi', 'eyeofdav', 'f', 'fabianatfp', 'fabric', 'fabul', 'facebook', 'facil', 'facilisim', 'facult', 'fairphon', 'falcon', 'fall', 'falling', 'falt', 'faltagraci', 'faltari', 'famili', 'fan', 'fanart', 'fanfics', 'fannygenguix', 'fans', 'fantasm', 'fantasylalig', 'far', 'faranduleroy', 'fatal', 'fav', 'favor', 'favorit', 'favst', 'fe', 'fea', 'fech', 'feeric', 'felic', 'felicit', 'felicitacion', 'felicitari', 'felipealcarazm', 'feliuvicent', 'feliz', 'felizlun', 'felizmart', 'felizojal', 'femihard', 'femimism', 'femin', 'feo', 'feralcaz', 'ferder', 'ferdiazgil', 'feri', 'fest', 'festival', 'fevaf', 'fi', 'ficcion', 'fich', 'fiebr', 'fiest', 'fiestasconamig', 'fif', 'figur', 'fij', 'filip', 'fill', 'fin', 'final', 'find', 'finisimaperson', 'finland', 'fio', 'firm', 'first', 'firstdat', 'flech', 'flexibl', 'flip', 'flipant', 'flipasss', 'floj', 'floki', 'flor', 'flyfap', 'flyintothestorm', 'fnac', 'foll', 'follow', 'followers', 'followexpress', 'foment', 'fond', 'fonzeff', 'for', 'foran', 'form', 'formalitosoy', 'fortun', 'fot', 'fotograf', 'fotografi', 'fotografxel', 'fotomil', 'fourty', 'fp', 'franc', 'franci', 'frandguez', 'fras', 'fre', 'frecuent', 'fredogf', 'frent', 'fresquit', 'fri', 'from', 'froot', 'frustrant', 'ftwd', 'fue', 'fueg', 'fuengirol', 'fuent', 'fuer', 'fueron', 'fuert', 'fuertesit', 'fuerz', 'fues', 'fugaz', 'fuim', 'fuisteis', 'fulgenlison', 'full', 'funcion', 'funcionsobreviv', 'fund', 'fundamental', 'funeral', 'funinfvneral', 'futbol', 'futboler', 'futur', 'fxnonam', 'gabiherranz', 'gabrielrufi', 'gaf', 'gal', 'galici', 'galleg', 'gallet', 'gallif', 'gamerxtrax', 'gaming', 'gan', 'ganador', 'gananci', 'gandi', 'gant', 'gaon', 'gappy', 'garabat', 'garbimuguruz', 'garcaterr', 'garfield', 'gargant', 'garrull', 'gat', 'gay', 'gb', 'gemacorg', 'gemel', 'gemeliers', 'gener', 'generacion', 'genial', 'gent', 'georgebleach', 'geralt', 'gestion', 'get', 'getwellsoonselenafromspain', 'ghazi', 'gig', 'gilipoll', 'gilipollec', 'gimnasi', 'girlprid', 'girotix', 'global', 'gloriatn', 'gloris', 'go', 'gobiern', 'goeth', 'gol', 'golp', 'gomin', 'gominolastrist', 'goonz', 'gord', 'goslum', 'gothicnekogd', 'gottabebravery', 'grab', 'graci', 'gracios', 'graffiter', 'gran', 'granctim', 'grand', 'grandisim', 'grasios', 'grassi', 'grat', 'gratis', 'gratuit', 'grav', 'graym', 'grazi', 'greenparadis', 'grieg', 'griezmann', 'grif', 'grim', 'grind', 'grit', 'grrab', 'grup', 'grupal', 'gsus', 'guad', 'guap', 'guapin', 'guapis', 'guapisim', 'guardi', 'guay', 'guayper', 'gudelj', 'guiastactic', 'guidobcor', 'guill', 'guillebouzon', 'guillermoterry', 'guin', 'gulag', 'gurpegui', 'gust', 'gustari', 'gutural', 'guzmannavarr', 'gzwik', 'h', 'ha', 'haas', 'hab', 'habeis', 'habi', 'habit', 'habl', 'hablab', 'hablais', 'hableconell', 'habr', 'habri', 'hac', 'haceis', 'hacert', 'haci', 'hackandbeers', 'hag', 'hagais', 'hahahahahahh', 'hall', 'hallazg', 'hambr', 'hamlet', 'han', 'hannibal', 'happyjungkookday', 'har', 'hari', 'harley', 'harleyftpreg', 'harrison', 'has', 'hast', 'hastag', 'haul', 'hausofmai', 'hav', 'hawkersc', 'hawklinecl', 'hawky', 'hay', 'hd', 'he', 'hecalledmeac', 'hech', 'heidegg', 'heikki', 'hel', 'helenacks', 'help', 'hem', 'her', 'herman', 'hermos', 'herod', 'heroinseb', 'heter', 'hey', 'hh', 'hic', 'hidrojing', 'hiel', 'hig', 'hij', 'hijodebilharzi', 'hijoest', 'hil', 'hipocrit', 'histor', 'histori', 'hiz', 'hmm', 'hol', 'hollagh', 'hom', 'hombr', 'homenaj', 'homofob', 'homrepordi', 'honor', 'hor', 'horari', 'horit', 'hormigazyt', 'horribl', 'hortalez', 'hosti', 'hous', 'hoy', 'hs', 'httpstcoamanssdaxi', 'httpstcobsrmwnrz', 'httpstcocidqqr', 'httpstcoctcteqv', 'httpstcodfkqasean', 'httpstcodicrghwex', 'httpstcodshopei', 'httpstcodtnjsrrt', 'httpstcohdoxoqgh', 'httpstcohlhyxox', 'httpstcohwqxwrjr', 'httpstcoiukuowsl', 'httpstcokhotodzc', 'httpstcokvpcwiigu', 'httpstcomnkkpif', 'httpstconammfnz', 'httpstcooldfpqcd', 'httpstcopgjqvxkyr', 'httpstcopwoxmffl', 'httpstcoqikgivz', 'httpstcorznuvpn', 'httpstcostuwlwbrk', 'httpstcotbkcvivo', 'httpstcotdhaguhgf', 'httpstcotsdgxnm', 'httpstcotyeghzkzy', 'httpstcoukycknu', 'httpstcovievst', 'httpstcowovpvtis', 'httpstcoxclhgsjeg', 'httpstcoxdvafgsp', 'httpstcoxgnkah', 'httpstcoyjbrmyk', 'httpstcozqctiobc', 'hub', 'huelv', 'huelvasevill', 'huert', 'hues', 'huev', 'hug', 'human', 'humor', 'humoryfot', 'hundredrooms', 'hurricaneoffir', 'huy', 'hym', 'hype', 'hyrul', 'hywzz', 'i', 'iagu', 'iazek', 'iba', 'ibas', 'ichdav', 'ichus', 'icon', 'iconic', 'id', 'ide', 'ideal', 'idealiceis', 'ideiazabaldu', 'identific', 'idiom', 'ido', 'ierrejon', 'ig', 'iglesi', 'ignaciojur', 'ignatiareilly', 'ignilefiasplash', 'ignor', 'igot', 'igual', 'ikutram', 'illo', 'iluminaughty', 'ilusion', 'imag', 'imagen', 'imagin', 'imaginat', 'iman', 'imanbl', 'imdariusbtch', 'imessag', 'imfebr', 'import', 'important', 'impos', 'impotent', 'imprescind', 'impresion', 'imprim', 'impuls', 'imunit', 'inacces', 'inamov', 'incantus', 'incit', 'inclu', 'incognit', 'increibl', 'independent', 'indign', 'indirect', 'inest', 'inestim', 'ineternet', 'inextric', 'infinit', 'inflad', 'info', 'infor', 'inform', 'informacion', 'informar', 'informativost', 'ingles', 'inhabilit', 'injust', 'inmens', 'inmigr', 'innat', 'innov', 'inocent', 'inolvid', 'insta', 'instagram', 'instal', 'instant', 'institut', 'insult', 'insurreccion', 'inteligent', 'intens', 'intent', 'inter', 'interes', 'interesnat', 'interestel', 'interior', 'internacional', 'internet', 'intomyhor', 'intrig', 'investidurarajoy', 'investidurarajoytv', 'investigar', 'inviern', 'invit', 'involu', 'involucr', 'ios', 'iphon', 'ir', 'ire', 'ireis', 'irem', 'iriswextall', 'irlesev', 'irme', 'iron', 'irrespetu', 'irrespons', 'irse', 'iruri', 'isabelsevill', 'isac', 'isameseguermtz', 'isasaweis', 'island', 'israelgv', 'itali', 'ivancastell', 'ivangomezruiz', 'ivnlz', 'iwananch', 'izdo', 'izquierd', 'jaaj', 'jabokatu', 'jackson', 'jaj', 'jajaj', 'jajajaj', 'jajajajaj', 'jajajajajaj', 'jajajajajajaj', 'jajajajajajajajajajajaj', 'jajajajajajajajajajajajaj', 'jajajajajajajjaajaj', 'jajajajjjaajjjjajaja', 'jajajj', 'jajajsjajajaj', 'jajaxd', 'jajsakjkjsadjjsjasjajaj', 'jam', 'janijoplin', 'janowicz', 'jap', 'japones', 'jardi', 'jarqu', 'jaul', 'jav', 'javi', 'javierdepec', 'javiturpeir', 'javivicgt', 'jcesarpl', 'jclilgangst', 'jcpblanc', 'jcponlin', 'jecaswords', 'jej', 'jejej', 'jejejej', 'jerez', 'jert', 'jesuismarc', 'jesusbengoeche', 'jimbr', 'jlblanc', 'jmallu', 'jmiguel', 'jo', 'joan', 'joanbaldovi', 'joantard', 'jod', 'jodi', 'jodidopreci', 'joes', 'johanagonzalezd', 'johann', 'jojojoj', 'jonathanfaus', 'jonbarand', 'jonor', 'jop', 'jordicanal', 'jorgegarzon', 'jorgelbu', 'jorgeon', 'jorgeruiz', 'joseangel', 'joseb', 'josebrit', 'josecarmelollb', 'josecg', 'josechejagg', 'josecs', 'joseemiii', 'joseydani', 'jovigobi', 'jpelirroj', 'jramonfernandez', 'juakyb', 'juan', 'juanalfons', 'juang', 'juanjoatleti', 'juanjok', 'juanrall', 'juapochohh', 'judithg', 'jueg', 'jug', 'juici', 'julenfenty', 'juli', 'juliaacouto', 'julioinsadji', 'juliojot', 'julwiki', 'junt', 'jura', 'just', 'justicehombr', 'justin', 'jvsantacreu', 'k', 'kalvanigh', 'kaori', 'karlit', 'katyklav', 'kayak', 'ke', 'kedadacan', 'keegs', 'keirz', 'kellydehar', 'kenne', 'kennysamuert', 'kespan', 'kikol', 'kill', 'kills', 'killytoki', 'kindermeli', 'kindl', 'king', 'kingkongjul', 'kioskoamm', 'kiwiliberal', 'kk', 'kmo', 'knekr', 'knifing', 'kopur', 'kostotrading', 'kousei', 'kriisvlc', 'kulusk', 'kunmader', 'kurokatz', 'kyli', 'l', 'la', 'lacalledeladi', 'lad', 'ladyeternals', 'ladyfrigopi', 'ladygraphen', 'ladykumari', 'ladylolit', 'ladypal', 'laf', 'lahierrit', 'lahoraalt', 'laic', 'lakarcel', 'lamecul', 'lament', 'lan', 'landscps', 'landspeci', 'lanz', 'lapollacalient', 'laponyr', 'lapsus', 'lar', 'larazon', 'larg', 'las', 'lasan', 'lastim', 'latinoamer', 'lau', 'laur', 'laurajovi', 'lav', 'lavuelt', 'lcasei', 'le', 'leal', 'learamartell', 'lech', 'lectur', 'lee', 'leem', 'leer', 'leerm', 'legal', 'leganesqu', 'legendari', 'legion', 'leicest', 'leid', 'leil', 'lejan', 'lejosdelmarfilm', 'lekilig', 'len', 'lenavg', 'lengu', 'lent', 'lentejit', 'lentesdecontact', 'leodicapri', 'leon', 'leonbenaventetw', 'leprosanom', 'les', 'lesbian', 'letinneverlxnd', 'levant', 'levanteemv', 'leyend', 'lguillermodepab', 'liam', 'liamv', 'libert', 'libr', 'librosdemari', 'lid', 'lidiawblog', 'lif', 'lig', 'ligael', 'lim', 'limit', 'lincoln', 'lind', 'link', 'linoon', 'lio', 'lionqueenofjah', 'lirond', 'list', 'literal', 'literari', 'litr', 'littlewhor', 'liverpool', 'livetodi', 'livingjacob', 'llam', 'llautet', 'llav', 'lleg', 'llegais', 'llegari', 'llego', 'llen', 'lleonardpl', 'llev', 'llevam', 'lleveis', 'llinars', 'llor', 'llorer', 'lluev', 'lmg', 'ln', 'lo', 'lobovcf', 'loc', 'local', 'lococamioner', 'logic', 'logr', 'lokimondrak', 'lol', 'lolap', 'lolhonor', 'lolspain', 'lom', 'lonchbox', 'loops', 'lor', 'lorainthepand', 'lorc', 'loretobaz', 'los', 'losers', 'losient', 'losspain', 'lostrending', 'lour', 'low', 'lozanoiren', 'ls', 'lucen', 'luch', 'luchador', 'luci', 'luciaaaypunt', 'luciaanagil', 'luciadollfac', 'luciakawaii', 'lucianoguer', 'luciaska', 'lucy', 'lueg', 'lug', 'lugnic', 'luis', 'luisangelmat', 'luiscallej', 'luisfbonill', 'lun', 'lupesb', 'lupus', 'luudelafuent', 'luvmneyparty', 'luz', 'lv', 'lvl', 'm', 'maaj', 'mac', 'macarron', 'machirul', 'machism', 'machorr', 'macr', 'mad', 'madeinleon', 'madison', 'madr', 'madrug', 'madrugon', 'madur', 'mafolu', 'mag', 'magaluf', 'magic', 'mahou', 'mait', 'maitecerez', 'maj', 'major', 'makna', 'mal', 'malag', 'malagacf', 'malaguen', 'mald', 'maldit', 'malenk', 'malet', 'malign', 'malit', 'mallorc', 'mamaheasesinadoaminovi', 'mamarrach', 'mamiauryn', 'mamm', 'man', 'manan', 'manch', 'mand', 'mandalabeach', 'mandam', 'mandar', 'mandari', 'maner', 'mangel', 'manifiest', 'manipul', 'mankind', 'manozom', 'mantec', 'mantendri', 'manu', 'manual', 'manubarb', 'manudeorleans', 'manudrugs', 'manuel', 'manuelacarmen', 'manueldezgali', 'manuelizqui', 'manujhidalg', 'map', 'mar', 'maravill', 'maravillosospor', 'marc', 'marcasgz', 'march', 'marcklebowski', 'mare', 'marg', 'margin', 'mari', 'marian', 'marianorajoy', 'maricarm', 'marii', 'mariiiiaperez', 'marinaarnald', 'marinaliaison', 'marinaytal', 'marioem', 'marionaral', 'maritim', 'marman', 'martiad', 'martiri', 'martublackr', 'marulang', 'maryansoy', 'marysays', 'mas', 'mascot', 'masculin', 'mason', 'masy', 'mat', 'matar', 'mate', 'material', 'matidiazwarrior', 'matricul', 'mavivieir', 'maxferib', 'maxisanchez', 'mayor', 'mayori', 'maz', 'mcaut', 'mccre', 'md', 'mdt', 'me', 'meand', 'med', 'medi', 'mediador', 'mediav', 'medic', 'medicin', 'medieval', 'medium', 'medus', 'meg', 'megh', 'mejican', 'mejor', 'mel', 'melen', 'meme', 'men', 'mencion', 'mendescod', 'menor', 'mensaj', 'mental', 'mentir', 'menu', 'menud', 'menudiari', 'meo', 'mer', 'merc', 'mercadolibr', 'mercadon', 'merec', 'mereei', 'merlu', 'mes', 'met', 'metaforacomparacion', 'metod', 'metr', 'mevoy', 'mg', 'mgw', 'mi', 'mia', 'mias', 'michael', 'michell', 'michellecrec', 'microaventur', 'microsoft', 'mid', 'mied', 'mientr', 'miercol', 'mierd', 'miguel', 'miguelagaiteir', 'miguelbarranc', 'miguelprat', 'miguelri', 'miguezyt', 'mil', 'milagr', 'milfncokieslol', 'millenium', 'millon', 'min', 'minadeid', 'minecraft', 'mingafri', 'ministeri', 'minori', 'minorityreport', 'minorizacion', 'minut', 'mio', 'mios', 'miquel', 'miquellis', 'mir', 'miradadelalun', 'miraqbi', 'mirar', 'mirart', 'mirat', 'miron', 'mis', 'miselizajan', 'miser', 'mism', 'mismoesport', 'misscatingcat', 'mit', 'mjdelri', 'mmmm', 'mmorpgs', 'mod', 'model', 'modest', 'mogollon', 'mol', 'molari', 'molecul', 'molest', 'molinon', 'molocebri', 'molon', 'moment', 'mon', 'monchoyt', 'mongol', 'monolog', 'monologu', 'monoton', 'monst', 'mont', 'montan', 'monton', 'montsegarci', 'monysit', 'mooooool', 'mor', 'moralzarzal', 'morbosaborealis', 'moren', 'mortal', 'mosc', 'mosquit', 'mostol', 'mot', 'motiv', 'motor', 'motril', 'mourinh', 'movi', 'movil', 'movimient', 'moz', 'mr', 'mrmedr', 'mrplayforliv', 'mrswxllac', 'msvelouri', 'mtvspain', 'mu', 'muacc', 'much', 'muchisim', 'muchs', 'muer', 'muerd', 'muert', 'mujer', 'mund', 'murci', 'music', 'musical', 'mutu', 'muwi', 'muy', 'muydemasiadopeculi', 'muyinteres', 'mxty', 'myendlesshazz', 'n', 'na', 'nac', 'nach', 'nacion', 'nad', 'nadi', 'nadin', 'nah', 'naitah', 'najera', 'nani', 'natural', 'navy', 'nba', 'neces', 'necesari', 'necesit', 'necesitoelviajeaandaluci', 'negoci', 'negr', 'neinyatt', 'neithanwolf', 'nenaaaaa', 'nenin', 'neodonnadi', 'ner', 'nere', 'nerearmcf', 'nereesantom', 'nervi', 'nervios', 'neuron', 'never', 'ni', 'niall', 'nicky', 'nicol', 'nigtwish', 'niievesgiil', 'nin', 'ningun', 'ninuc', 'nioh', 'nive', 'nivel', 'nmlss', 'no', 'noahor', 'noch', 'nocturn', 'noj', 'nombr', 'nomin', 'non', 'nonn', 'nooo', 'nop', 'norm', 'normal', 'normaruiz', 'norteamerican', 'nos', 'noseasheter', 'nosotr', 'not', 'notarioalcal', 'notasdelmisteri', 'notici', 'notificacion', 'nouri', 'novat', 'novatillototal', 'novedad', 'novi', 'noviembr', 'nozomi', 'ntros', 'nu', 'nub', 'nubl', 'nuci', 'nuestr', 'nuev', 'nuevatribun', 'nul', 'numer', 'nunc', 'nuristill', 'nutell', 'o', 'objet', 'objetivospart', 'oblig', 'obligart', 'obligatori', 'obten', 'obvi', 'ocho', 'oclin', 'octubr', 'ocurr', 'odi', 'odise', 'oest', 'officialcritik', 'officialsaik', 'oficial', 'ofrec', 'oh', 'ohgabi', 'ohhh', 'ohhhhh', 'ohmyworld', 'ohwowfuckyou', 'oia', 'oie', 'ojal', 'ojerafarloper', 'ojo', 'ojos', 'oju', 'ok', 'ola', 'ole', 'olimp', 'olivar', 'olivaresjavi', 'olmill', 'olor', 'olvid', 'omar', 'omeg', 'omegaesp', 'omg', 'omrdh', 'ono', 'opac', 'opcion', 'open', 'operacion', 'opin', 'opinion', 'oprim', 'optim', 'optometri', 'opuest', 'oqu', 'organiz', 'orgull', 'oriannasenpaii', 'original', 'orionpirat', 'orish', 'orzuel', 'os', 'oscarbv', 'ose', 'osti', 'ot', 'otak', 'otaku', 'otegi', 'otonal', 'otra', 'otras', 'otro', 'otros', 'oud', 'ovari', 'over', 'overgameorg', 'ow', 'oye', 'p', 'pa', 'pabl', 'pabloserran', 'pacienci', 'pacient', 'pacm', 'pacojon', 'paconadal', 'pacopiner', 'pact', 'paddockgpworld', 'padel', 'padr', 'paellasdeverd', 'pag', 'pagin', 'paintball', 'pais', 'pajarit', 'palabr', 'palanz', 'palm', 'palomacv', 'palomaeb', 'palomit', 'pam', 'pamplon', 'panadespb', 'panchogustav', 'pandeliri', 'panoram', 'pantalon', 'pantan', 'panuel', 'papelitoroj', 'par', 'parader', 'parafras', 'paranormal', 'parar', 'parec', 'pareci', 'parej', 'parezc', 'parip', 'parodi', 'parqu', 'parsimony', 'part', 'particip', 'partidari', 'partidopacm', 'pas', 'pasaport', 'pase', 'passionenglish', 'pastel', 'pastill', 'pastoret', 'pat', 'patet', 'patriciareyescs', 'pattycavallon', 'pau', 'paul', 'paulaku', 'pazborisb', 'pc', 'pcs', 'pd', 'pdbuen', 'pe', 'peas', 'peaz', 'pec', 'ped', 'pedagogi', 'pedant', 'pedrojc', 'peeeer', 'peg', 'peichsantan', 'pel', 'pele', 'peli', 'pelicul', 'peliroj', 'pelis', 'pelocon', 'pelot', 'pelotoncarrefour', 'pen', 'pendient', 'peniscol', 'pens', 'pensamient', 'pensanf', 'penuri', 'peor', 'pepbuen', 'pepit', 'pequen', 'per', 'perd', 'perdi', 'perdon', 'perdonam', 'perfect', 'perfil', 'peric', 'periodist', 'permis', 'permit', 'peroquedicesss', 'perr', 'perriestheory', 'perrit', 'person', 'personaj', 'personal', 'pervers', 'pes', 'pesadit', 'pesim', 'pet', 'peters', 'pewdiepi', 'pf', 'pgimenezfuent', 'pic', 'picadur', 'pid', 'pie', 'piedr', 'piel', 'piens', 'pierd', 'piern', 'pies', 'pij', 'pijam', 'pill', 'pillais', 'pilot', 'pimpsokzi', 'pint', 'pintaz', 'pinterest', 'pioner', 'pir', 'pirataroberts', 'pirrakasipl', 'pis', 'piscin', 'pist', 'pitas', 'pitdiamondcor', 'pitorr', 'plac', 'plad', 'plan', 'planaz', 'planet', 'planetainquiet', 'planetaradi', 'plant', 'plantadosesp', 'plat', 'platic', 'platon', 'play', 'players', 'playstation', 'plaz', 'ple', 'plen', 'pletor', 'pll', 'pls', 'plsnomoredram', 'plum', 'plural', 'po', 'pobr', 'pobresit', 'pobrezacer', 'poc', 'pod', 'podeis', 'poder', 'podr', 'podreis', 'podri', 'podriais', 'poem', 'poitic', 'pokecas', 'pokemon', 'pokemong', 'polici', 'polifack', 'polit', 'poluxprm', 'pon', 'ponert', 'ponferr', 'pong', 'poni', 'poquill', 'poquit', 'por', 'pork', 'porn', 'porq', 'porqu', 'portalsbudlog', 'portavoz', 'portonov', 'pos', 'posibil', 'posibl', 'posicion', 'positiv', 'post', 'postits', 'posttertuli', 'postunchart', 'postur', 'pot', 'potasi', 'potencial', 'potent', 'pp', 'pq', 'practic', 'prat', 'preci', 'precios', 'precipici', 'prefer', 'preferent', 'pregunt', 'premi', 'preocup', 'preocupeis', 'prepar', 'preparat', 'present', 'president', 'presion', 'prest', 'presum', 'previ', 'prim', 'primer', 'principi', 'pring', 'priv', 'pro', 'prob', 'probabl', 'probable', 'problem', 'proces', 'product', 'profesion', 'profesional', 'profesor', 'profund', 'program', 'prohib', 'promet', 'promocion', 'pront', 'prontit', 'propi', 'propon', 'propuest', 'provoc', 'prox', 'proxim', 'proyect', 'prueb', 'prxttytimebomb', 'przupp', 'ps', 'pseudofemin', 'psicologi', 'psicomotric', 'pso', 'psychoviolenc', 'ptraciogg', 'public', 'pud', 'puebl', 'pueblesit', 'pued', 'pueh', 'puert', 'puertoanagaplaz', 'pues', 'puest', 'puffff', 'pul', 'pulser', 'puni', 'punt', 'puntal', 'puntit', 'puntodevictori', 'pur', 'pus', 'put', 'putaqu', 'putis', 'puunissh', 'pvay', 'q', 'qier', 'qsf', 'qt', 'que', 'qued', 'quedais', 'queduroesseradult', 'quejic', 'quemadisim', 'quer', 'querais', 'quereis', 'queri', 'quesadac', 'qui', 'quien', 'quier', 'quill', 'quinn', 'quit', 'quitarmel', 'quiz', 'r', 'rabi', 'rabiekhd', 'rachelgr', 'rachelnichols', 'radi', 'radicaiizat', 'radioh', 'radioskylab', 'rafahern', 'rafalas', 'rafamart', 'raistlin', 'rajoy', 'ranlemontral', 'ranlun', 'rap', 'raquel', 'raquellpz', 'rar', 'rarroch', 'rastreador', 'rat', 'ratgull', 'ratit', 'raton', 'raulmjimenez', 'ray', 'raz', 'rcde', 'rdelvall', 'reaccion', 'real', 'realid', 'realiz', 'rebecarus', 'recam', 'recet', 'recib', 'recom', 'recomend', 'recomendacion', 'recomendais', 'recomiend', 'reconoc', 'record', 'recordar', 'recordatori', 'recuerd', 'recuper', 'recuperacion', 'red', 'redskyf', 'reduc', 'reembols', 'reencuentr', 'reenvi', 'referi', 'refier', 'reflector', 'reform', 'reforz', 'refut', 'regl', 'reglamentari', 'regres', 'rehag', 'reid', 'rein', 'reinadelosgat', 'reir', 'reivajcorr', 'reivindicacion', 'relatosgregori', 'rem', 'remak', 'remardifuck', 'remasteriz', 'ren', 'renac', 'rend', 'renriesg', 'reo', 'repasit', 'repelent', 'repet', 'repit', 'represent', 'reptarazul', 'reserv', 'resign', 'respet', 'respingon', 'respond', 'respuest', 'rest', 'restaurantecasinolaspalm', 'resuelv', 'result', 'resum', 'retoqu', 'retras', 'retrat', 'retrotrain', 'retwitte', 'rev', 'revellin', 'revent', 'reventaor', 'revient', 'rey', 'reybult', 'reyminguel', 'rfalqu', 'ria', 'ric', 'rickyxcx', 'ridicul', 'rie', 'rigur', 'riley', 'rincon', 'rind', 'rio', 'ris', 'rival', 'river', 'rivi', 'rivier', 'riz', 'rlegaz', 'rob', 'robert', 'robot', 'robots', 'rodill', 'rodrig', 'roenlar', 'roj', 'roll', 'rollaz', 'roman', 'rompeis', 'ronancok', 'ronchon', 'rorscharg', 'ros', 'rosend', 'rossiepussi', 'rot', 'rotond', 'rottweil', 'roussirt', 'royaldarjeeling', 'rsdspain', 'rt', 'rts', 'rub', 'rubi', 'rubiu', 'ruedecit', 'ruestern', 'rulin', 'rus', 'rushsmith', 'rusi', 'russel', 'rut', 'ruthtoledan', 'rutin', 'ryfsw', 's', 'sab', 'sabadet', 'sabeis', 'sabi', 'sabr', 'sabrin', 'sabrygarciaren', 'sac', 'sad', 'saezcristin', 'saezmanu', 'sag', 'sailortesserei', 'sakuraabril', 'sal', 'saldr', 'salg', 'sali', 'salirt', 'salmon', 'salom', 'salomeper', 'salon', 'salt', 'salu', 'salud', 'saludais', 'salv', 'sammyriotmars', 'san', 'sanchezdelreal', 'sandraramm', 'sandyxyxx', 'sangues', 'santic', 'santizuz', 'sao', 'saqu', 'sar', 'sarcasm', 'satur', 'scroll', 'sdasldjalsk', 'se', 'sea', 'sean', 'seas', 'sectari', 'sed', 'seelphy', 'seestren', 'seestrenasietev', 'segu', 'seguimient', 'seguir', 'segun', 'segund', 'segur', 'seis', 'selenepetal', 'seman', 'semifinal', 'sen', 'sencill', 'senioresfer', 'senor', 'sensacin', 'sensacion', 'sensibil', 'sensibl', 'sent', 'sentimental', 'sentimient', 'sentir', 'sep', 'separ', 'separat', 'sepron', 'septiembr', 'sepul', 'ser', 'serbailop', 'sergiiomoya', 'sergiosmil', 'seri', 'seriais', 'serioprim', 'serl', 'sesderm', 'sesion', 'sesiondeinvestidur', 'set', 'sev', 'severinoiezzi', 'severus', 'sevezubiri', 'sevill', 'sevillan', 'sexist', 'sexy', 'shakespear', 'shark', 'shawn', 'shawnupdatess', 'shay', 'sheer', 'shielledfox', 'shippers', 'shit', 'shrrymh', 'si', 'sid', 'siempr', 'siemprw', 'siend', 'sient', 'sientet', 'siest', 'sig', 'signif', 'sigu', 'sigui', 'siguiendot', 'siguient', 'siiii', 'siiiii', 'siiiiii', 'silenc', 'silenci', 'sill', 'silmarillion', 'silversunrhythm', 'silvi', 'simpat', 'simpl', 'simplement', 'sin', 'sinc', 'sincer', 'sindicat', 'sindrom', 'sinverguenzahay', 'siqu', 'sirdeschain', 'sirv', 'sis', 'sismic', 'sistem', 'sistematm', 'sitgesfestival', 'siti', 'situacion', 'skype', 'skyruss', 'slammactiva', 'slaveofjavi', 'slcuerv', 'slowdanc', 'snap', 'snipshit', 'snpachats', 'sobr', 'sobredosis', 'sobretod', 'sobreviv', 'soci', 'sociabl', 'social', 'sof', 'sofi', 'soi', 'sois', 'sol', 'solidar', 'solidarizart', 'solit', 'solter', 'solucion', 'som', 'somer', 'son', 'sonri', 'sonris', 'sonrisachocolat', 'sony', 'soport', 'sorayitapush', 'sorendix', 'sorprend', 'sorpres', 'sorr', 'sorry', 'sorte', 'sortij', 'soryrah', 'sosagraphs', 'sosteng', 'soundcloud', 'soundtrack', 'soy', 'soyalt', 'spam', 'spaniart', 'sparks', 'sper', 'spherasports', 'spidey', 'spiegelspik', 'spoil', 'spoti', 'spotify', 'springst', 'sprit', 'srhador', 'srmerlus', 'srmussel', 'sta', 'startups', 'stas', 'stats', 'stellonecthun', 'still', 'stor', 'strang', 'stream', 'su', 'sub', 'subaru', 'subcampeon', 'subi', 'subiri', 'subnormal', 'subsan', 'suci', 'sucumb', 'sud', 'sudader', 'suel', 'suelt', 'suen', 'suert', 'sufr', 'sugerenci', 'sugerent', 'suicid', 'sum', 'sumiesb', 'summ', 'sup', 'super', 'superfici', 'superficial', 'superior', 'superpaperend', 'supershor', 'supong', 'supps', 'supuest', 'surgi', 'surr', 'surreal', 'sus', 'susanatruiz', 'sushunet', 'suspend', 'suspir', 'sust', 'sutil', 'suy', 'svvvdbxxy', 'sweetbiix', 'sweetsuricat', 't', 'ta', 'taa', 'tabernit', 'tagg', 'tague', 'tal', 'talent', 'talents', 'tall', 'tambi', 'tambor', 'tampoc', 'tan', 'tant', 'tantisim', 'tap', 'tapader', 'tard', 'tardari', 'tardenoch', 'tardeparalair', 'targary', 'tarjet', 'tartadedurazn', 'tas', 'tatto', 'tb', 'te', 'teabrounanuev', 'tecnic', 'teen', 'tej', 'tejecristal', 'tel', 'teldccxox', 'telecinc', 'telefer', 'telefon', 'telesurtv', 'tem', 'temor', 'templ', 'tempor', 'temporal', 'tempran', 'ten', 'tendenci', 'tendr', 'tendreis', 'tendri', 'teneis', 'teng', 'tengomasrazon', 'teni', 'tenistachanel', 'teogarciaege', 'teori', 'terapi', 'tercer', 'termin', 'terremot', 'terror', 'tet', 'text', 'th', 'thanks', 'the', 'thelordofkai', 'themindcamp', 'thks', 'ti', 'tia', 'tias', 'tidxs', 'tiemp', 'tien', 'tiend', 'tierr', 'tierramagn', 'tifuh', 'timelin', 'tint', 'tio', 'tiov', 'tip', 'tipic', 'tipsterapuest', 'tir', 'titul', 'titular', 'tl', 'tlyo', 'to', 'tobill', 'tobogan', 'toby', 'toc', 'tocamat', 'tocay', 'tod', 'todavi', 'todomegadriv', 'tods', 'toled', 'tolerd', 'tom', 'tomasimedi', 'tomasinzgz', 'ton', 'tong', 'toni', 'toniend', 'tonithrowdown', 'tont', 'tontonin', 'tonyysorayaenmalag', 'too', 'top', 'toret', 'torment', 'torne', 'tortill', 'total', 'tots', 'tozudez', 'tra', 'trabaj', 'trabajadors', 'trafic', 'trag', 'trail', 'trainor', 'tranquil', 'tras', 'trav', 'trayectori', 'trech', 'tren', 'trendingestren', 'tres', 'triatlon', 'tricks', 'trimestr', 'trinitr', 'tripl', 'triplet', 'trist', 'triunf', 'trocit', 'troi', 'trokist', 'tron', 'ttlemans', 'tu', 'tubex', 'tuit', 'tunez', 'tups', 'turismoasturi', 'turubruj', 'tus', 'tuv', 'tuy', 'tv', 'tw', 'twd', 'tweet', 'twitt', 'twitter', 'txemacg', 'txemadomi', 'txispitabi', 'tyuc', 'uby', 'uf', 'ugmadr', 'ukelel', 'ulpgc', 'ultim', 'umaprepon', 'un', 'una', 'unas', 'unaustrac', 'unculemadr', 'unic', 'unicovy', 'univ', 'univers', 'uno', 'unos', 'unpase', 'ursulabay', 'us', 'usar', 'usarl', 'usas', 'usernam', 'uso', 'usted', 'usuari', 'utiliz', 'uu', 'va', 'vacacion', 'vacasuec', 'vaci', 'vag', 'val', 'valabin', 'valdr', 'valenci', 'valenciacf', 'valenciaplaz', 'vall', 'vallec', 'vallespasieg', 'valor', 'vam', 'vampyresmovi', 'van', 'vanderhast', 'vap', 'vari', 'varitoponchit', 'vas', 'vasc', 'vay', 'vcfcre', 'vd', 've', 'vea', 'veais', 'vec', 'vegan', 'vegett', 'vehicul', 'vei', 'veis', 'veloc', 'veloz', 'vem', 'ven', 'vend', 'vendr', 'veng', 'veni', 'ventan', 'veo', 'ver', 'veran', 'verbalfis', 'verd', 'verdad', 'verdader', 'verdaderx', 'vergonz', 'verl', 'verm', 'veronik', 'veronikamoral', 'veronikmlg', 'verosl', 'vers', 'versem', 'version', 'vert', 'very', 'ves', 'vespacitym', 'vest', 'vet', 'veta', 'veucat', 'vez', 'vi', 'via', 'viaj', 'vicentmoreravcf', 'victorerubi', 'victorypeac', 'vid', 'vide', 'viej', 'vien', 'viend', 'viern', 'vig', 'vigarcu', 'viii', 'vikolh', 'villavici', 'vinagr', 'vinet', 'violenci', 'violetatrujill', 'virginiadlp', 'virsnr', 'visibl', 'visit', 'vist', 'vital', 'viv', 'vlog', 'vma', 'vmas', 'vmc', 'vocal', 'vodafon', 'voicesyntex', 'vol', 'volv', 'volver', 'vosotr', 'vot', 'votant', 'voy', 'voz', 'vs', 'vuel', 'vuelt', 'vuelv', 'vuestr', 'wa', 'wait', 'waltergenial', 'waltersobchak', 'wap', 'wasaps', 'weastbrook', 'web', 'wen', 'weyenseri', 'wgwillyg', 'whatev', 'wifi', 'wikkedchild', 'wildhat', 'will', 'willy', 'willyrexyt', 'willytoled', 'willytolerdo', 'wilmalorenz', 'wingedarrow', 'winterscliffrd', 'wolf', 'work', 'worth', 'x', 'xavigarrig', 'xcoors', 'xd', 'xdd', 'xdddd', 'xenomorf', 'xik', 'ximnart', 'ximovr', 'xoancarv', 'xoshir', 'xq', 'xqwcrp', 'xsetharmy', 'xslgy', 'xzdry', 'y', 'ya', 'yaa', 'yagoperrin', 'yanke', 'yasmimendegui', 'yddeon', 'yeoldenemesis', 'yessic', 'yeyiximmm', 'yih', 'ykony', 'yo', 'yolandayy', 'yoohookan', 'yoooo', 'yoshi', 'yosiempr', 'yosmath', 'you', 'youtub', 'yr', 'yulianpo', 'yuniiiiito', 'zapd', 'zexioning', 'zgz', 'ziml', 'zknowcks', 'zoestrang', 'zon', 'zonammorpg', 'zumbadisim']\n",
            "(1008, 3746)\n",
            "  (0, 2221)\t0.10941088516835301\n",
            "  (0, 2883)\t0.08174236885336657\n",
            "  (0, 2386)\t0.3238752078024629\n",
            "  (0, 0)\t0.10247260809749327\n",
            "  (0, 2874)\t0.22536065344980744\n",
            "  (0, 2129)\t0.18631627515959595\n",
            "  (0, 1167)\t0.26171944430154004\n",
            "  (0, 299)\t0.2677064452536423\n",
            "  (0, 3192)\t0.23064156255107862\n",
            "  (0, 3333)\t0.3238752078024629\n",
            "  (0, 1570)\t0.3238752078024629\n",
            "  (0, 1520)\t0.18528551928418419\n",
            "  (0, 2884)\t0.19554583848765864\n",
            "  (0, 2921)\t0.2517146010364793\n",
            "  (0, 1054)\t0.10903725045708237\n",
            "  (0, 651)\t0.3238752078024629\n",
            "  (0, 86)\t0.2517146010364793\n",
            "  (0, 1799)\t0.24744128278510705\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aRMO5YJkBK0B",
        "colab_type": "code",
        "outputId": "2593ec9f-e37d-47b6-9f05-a885d4f0700e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "cell_type": "code",
      "source": [
        "# HashingVectorizer WORD\n",
        "vectorizer = HashingVectorizer(n_features= 2 ** 22,\n",
        "                             analyzer='word',\n",
        "                             ngram_range = (1,1),\n",
        "                             strip_accents = \"ascii\",\n",
        "                             lowercase = True)\n",
        "\n",
        "train_vectors = vectorizer.fit_transform(trainCorpus)\n",
        "dev_vectors = vectorizer.transform(devCorpus)\n",
        "\n",
        "\n",
        "print(train_vectors.shape)\n",
        "\n",
        "print(train_vectors[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1008, 4194304)\n",
            "  (0, 57778)\t0.25\n",
            "  (0, 203536)\t0.25\n",
            "  (0, 502360)\t-0.25\n",
            "  (0, 642616)\t-0.25\n",
            "  (0, 843554)\t-0.25\n",
            "  (0, 1157025)\t0.25\n",
            "  (0, 1300924)\t0.25\n",
            "  (0, 1347213)\t0.25\n",
            "  (0, 1585224)\t0.25\n",
            "  (0, 1665705)\t0.25\n",
            "  (0, 1863624)\t-0.25\n",
            "  (0, 2007332)\t-0.25\n",
            "  (0, 2312478)\t-0.25\n",
            "  (0, 2742171)\t0.25\n",
            "  (0, 3158444)\t0.25\n",
            "  (0, 3570582)\t0.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "S7drztnKDFqT",
        "colab_type": "code",
        "outputId": "7bc53949-a914-475b-9310-c2f7bbc16abe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        }
      },
      "cell_type": "code",
      "source": [
        "# HashingVectorizer CHAR\n",
        "vectorizer = HashingVectorizer(n_features= 2 ** 18,\n",
        "                             analyzer='char_wb',\n",
        "                             ngram_range = (1,5),\n",
        "                             strip_accents = \"ascii\",\n",
        "                             lowercase = True)\n",
        "\n",
        "train_vectors = vectorizer.fit_transform(trainCorpus)\n",
        "dev_vectors = vectorizer.transform(devCorpus)\n",
        "\n",
        "\n",
        "print(train_vectors.shape)\n",
        "\n",
        "print(train_vectors[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1008, 262144)\n",
            "  (0, 344)\t0.020219563399637347\n",
            "  (0, 1465)\t0.020219563399637347\n",
            "  (0, 1474)\t-0.020219563399637347\n",
            "  (0, 4203)\t0.020219563399637347\n",
            "  (0, 4211)\t-0.020219563399637347\n",
            "  (0, 4697)\t0.020219563399637347\n",
            "  (0, 5298)\t0.040439126799274694\n",
            "  (0, 5468)\t0.020219563399637347\n",
            "  (0, 6324)\t0.020219563399637347\n",
            "  (0, 7793)\t-0.020219563399637347\n",
            "  (0, 7841)\t0.040439126799274694\n",
            "  (0, 8294)\t-0.040439126799274694\n",
            "  (0, 8371)\t-0.040439126799274694\n",
            "  (0, 9661)\t-0.020219563399637347\n",
            "  (0, 10604)\t-0.020219563399637347\n",
            "  (0, 11389)\t0.08087825359854939\n",
            "  (0, 12136)\t-0.020219563399637347\n",
            "  (0, 12360)\t0.040439126799274694\n",
            "  (0, 13348)\t-0.12131738039782408\n",
            "  (0, 13655)\t-0.020219563399637347\n",
            "  (0, 14428)\t0.08087825359854939\n",
            "  (0, 15072)\t0.08087825359854939\n",
            "  (0, 15164)\t-0.020219563399637347\n",
            "  (0, 15271)\t-0.020219563399637347\n",
            "  (0, 16260)\t0.020219563399637347\n",
            "  :\t:\n",
            "  (0, 240561)\t0.020219563399637347\n",
            "  (0, 241254)\t0.020219563399637347\n",
            "  (0, 242668)\t0.020219563399637347\n",
            "  (0, 244019)\t-0.020219563399637347\n",
            "  (0, 244474)\t-0.020219563399637347\n",
            "  (0, 245130)\t-0.020219563399637347\n",
            "  (0, 245530)\t0.020219563399637347\n",
            "  (0, 246034)\t-0.040439126799274694\n",
            "  (0, 247370)\t-0.020219563399637347\n",
            "  (0, 247436)\t0.020219563399637347\n",
            "  (0, 247921)\t0.020219563399637347\n",
            "  (0, 248149)\t-0.020219563399637347\n",
            "  (0, 248700)\t0.020219563399637347\n",
            "  (0, 249852)\t-0.020219563399637347\n",
            "  (0, 251214)\t-0.020219563399637347\n",
            "  (0, 252348)\t0.020219563399637347\n",
            "  (0, 253652)\t0.06065869019891204\n",
            "  (0, 254013)\t0.020219563399637347\n",
            "  (0, 255809)\t0.020219563399637347\n",
            "  (0, 257034)\t0.020219563399637347\n",
            "  (0, 257146)\t0.020219563399637347\n",
            "  (0, 258588)\t0.020219563399637347\n",
            "  (0, 260324)\t0.020219563399637347\n",
            "  (0, 261642)\t0.020219563399637347\n",
            "  (0, 261648)\t0.020219563399637347\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "k6yHyVCTD8kg",
        "colab_type": "code",
        "outputId": "d83b24e1-73a4-4389-c647-f86154150754",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        }
      },
      "cell_type": "code",
      "source": [
        "# CountVectorizer Char\n",
        "vectorizer = CountVectorizer(min_df=1,\n",
        "                             max_df = 0.9,\n",
        "                             analyzer='char_wb',\n",
        "                             ngram_range = (2,6),\n",
        "                             max_features = 50000,\n",
        "                             strip_accents = \"ascii\",\n",
        "                             lowercase = True)\n",
        "\n",
        "train_vectors = vectorizer.fit_transform(trainCorpus)\n",
        "dev_vectors = vectorizer.transform(devCorpus)\n",
        "\n",
        "\n",
        "print(train_vectors.shape)\n",
        "\n",
        "print(train_vectors[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1008, 50000)\n",
            "  (0, 12910)\t1\n",
            "  (0, 12913)\t1\n",
            "  (0, 25498)\t2\n",
            "  (0, 5199)\t1\n",
            "  (0, 25493)\t1\n",
            "  (0, 12909)\t2\n",
            "  (0, 25496)\t2\n",
            "  (0, 5196)\t1\n",
            "  (0, 12892)\t1\n",
            "  (0, 12905)\t2\n",
            "  (0, 25492)\t3\n",
            "  (0, 5195)\t1\n",
            "  (0, 12891)\t3\n",
            "  (0, 25490)\t3\n",
            "  (0, 5188)\t1\n",
            "  (0, 12884)\t3\n",
            "  (0, 2505)\t1\n",
            "  (0, 12730)\t1\n",
            "  (0, 2504)\t1\n",
            "  (0, 24109)\t1\n",
            "  (0, 12729)\t1\n",
            "  (0, 2498)\t1\n",
            "  (0, 24108)\t1\n",
            "  (0, 12703)\t1\n",
            "  (0, 43711)\t1\n",
            "  :\t:\n",
            "  (0, 25762)\t2\n",
            "  (0, 13052)\t1\n",
            "  (0, 2338)\t2\n",
            "  (0, 18723)\t1\n",
            "  (0, 6143)\t1\n",
            "  (0, 5605)\t1\n",
            "  (0, 26621)\t1\n",
            "  (0, 13712)\t1\n",
            "  (0, 5187)\t1\n",
            "  (0, 6440)\t1\n",
            "  (0, 36454)\t1\n",
            "  (0, 6439)\t2\n",
            "  (0, 45776)\t1\n",
            "  (0, 36453)\t2\n",
            "  (0, 6438)\t2\n",
            "  (0, 45775)\t2\n",
            "  (0, 36452)\t2\n",
            "  (0, 6436)\t2\n",
            "  (0, 22308)\t1\n",
            "  (0, 2850)\t1\n",
            "  (0, 5603)\t2\n",
            "  (0, 22919)\t2\n",
            "  (0, 26827)\t1\n",
            "  (0, 20460)\t2\n",
            "  (0, 26826)\t2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "c7RWRLmHEd0Q",
        "colab_type": "code",
        "outputId": "267e18ee-37ee-4176-bbf5-47f44e80f7ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "cell_type": "code",
      "source": [
        "# CountVectorizer Word\n",
        "vectorizer = CountVectorizer(min_df=1,\n",
        "                             max_df = 0.7,\n",
        "                             analyzer='word',\n",
        "                             ngram_range = (1,1),\n",
        "                             max_features = 50000,\n",
        "                             strip_accents = \"ascii\",\n",
        "                             lowercase = True)\n",
        "\n",
        "train_vectors = vectorizer.fit_transform(trainCorpus)\n",
        "dev_vectors = vectorizer.transform(devCorpus)\n",
        "\n",
        "\n",
        "print(train_vectors.shape[1])\n",
        "\n",
        "print(train_vectors[1])"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1008, 4791)\n",
            "  (0, 2307)\t1\n",
            "  (0, 197)\t1\n",
            "  (0, 880)\t1\n",
            "  (0, 1405)\t1\n",
            "  (0, 3751)\t1\n",
            "  (0, 3691)\t1\n",
            "  (0, 1986)\t1\n",
            "  (0, 2068)\t1\n",
            "  (0, 4265)\t1\n",
            "  (0, 4094)\t1\n",
            "  (0, 1548)\t1\n",
            "  (0, 2693)\t1\n",
            "  (0, 3682)\t1\n",
            "  (0, 3010)\t1\n",
            "  (0, 3688)\t1\n",
            "  (0, 2796)\t1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hLA4M7ObnSi9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b2a701f5-c972-42bc-cca0-59b40229cf3e"
      },
      "cell_type": "code",
      "source": [
        "def dict_tweet (tweets):\n",
        "  res = []\n",
        "  for tweet in tweets:\n",
        "    neg = 0;\n",
        "    pos = 0;\n",
        "\n",
        "    #print (tweet)\n",
        "    words = tweet.split(' ')\n",
        "    for word in words:\n",
        "      word = word.replace('-', '')\n",
        "      word = word.replace('\\n', '')\n",
        "      word = word.replace('.', '')\n",
        "      word = word.replace(',', '')\n",
        "      word = word.replace('!', '')\n",
        "      word = word.replace('\"\"', '')\n",
        "      word = word.replace('#', '')\n",
        "      word = word.replace('*', '')\n",
        "      word = word.replace('@', '')\n",
        "      word = word.lower()\n",
        "      if word in Poldict:\n",
        "        if Poldict[word] == \"negative\":\n",
        "          neg += 1\n",
        "        if Poldict[word] == \"positive\":\n",
        "           pos += 1\n",
        "    total=neg+pos\n",
        "    res.append(pos-neg)\n",
        "    '''\n",
        "    if(total>0):\n",
        "      res.append({'p' : pos/total, 'n' : neg/total})\n",
        "    else:\n",
        "      res.append({'p' :0, 'n' : 0})\n",
        "    '''\n",
        "  return res\n",
        "#res = dict_tweet(['-Me caes muy bien \\n-Tienes que jugar más partidas al lol con Russel y conmigo\\n-Por qué tan Otako, deja de ser otako\\n-Haber si me muero', '@myendlesshazza a. que puto mal escribo\\n\\nb. me sigo surrando help \\n\\n3. ha quedado raro el \"cómetelo\" ahí JAJAJAJA', '@estherct209 jajajaja la tuya y la d mucha gente seguro!! Pero yo no puedo sin mi melena me muero '])\n",
        "res = dict_tweet(trainCorpus)\n",
        "#print(res)\n",
        "\n",
        "correcto = 0\n",
        "incorrecto = 0\n",
        "neutral = 0\n",
        "for i in range(0, len(trainCorpus)):\n",
        "  #print(res[i])\n",
        "  #print(train_labels[i])\n",
        "  \n",
        "  if res[i] < 0 and train_labels[i]==\"N\":\n",
        "     correcto += 1\n",
        "  elif res[i] > 0 and train_labels[i]==\"P\":\n",
        "    correcto += 1\n",
        "  elif res[i] == 0:\n",
        "    neutral +=1\n",
        "  else:\n",
        "    incorrecto += 1\n",
        "    \n",
        "print (\"Correcto \"+str(correcto))\n",
        "print (\"Incorrecto \"+str(incorrecto))\n",
        "print (\"Neutral \"+str(neutral))\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correcto 401\n",
            "Incorrecto 261\n",
            "Neutral 346\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "r8S6h6rIyjnb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "807386ed-2b7c-48e9-de5d-0a524c05004f"
      },
      "cell_type": "code",
      "source": [
        "text_clf = Pipeline([\n",
        "    ('vect', TfidfVectorizer(min_df=1,\n",
        "                             max_df = 0.99,\n",
        "                             sublinear_tf=False,\n",
        "                             analyzer='word',\n",
        "                             ngram_range = (1,1),\n",
        "                             max_features = None,\n",
        "                             strip_accents = \"ascii\",\n",
        "                             lowercase = True,\n",
        "                             smooth_idf = True,\n",
        "                             use_idf=False)),\n",
        "    \n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', svm.LinearSVC(penalty='l2',\n",
        "                          loss='squared_hinge',\n",
        "                          dual=True,\n",
        "                          tol=0.0001, \n",
        "                          C=1.0, \n",
        "                          multi_class='ovr',\n",
        "                          fit_intercept=True,\n",
        "                          intercept_scaling=1,\n",
        "                          class_weight=None,\n",
        "                          verbose=0,\n",
        "                          random_state=None,\n",
        "                          max_iter=1000)),\n",
        "])\n",
        "\n",
        "text_clf.fit(trainCorpus, train_labels)  \n",
        "\n",
        "predicted = text_clf.predict(devCorpus)\n",
        "np.mean(predicted == dev_labels)            \n",
        "\n",
        "\n",
        "print(classification_report(dev_labels, predicted))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           N       0.59      0.82      0.69       219\n",
            "         NEU       0.33      0.12      0.17        69\n",
            "        NONE       0.42      0.18      0.25        62\n",
            "           P       0.60      0.59      0.59       156\n",
            "\n",
            "   micro avg       0.57      0.57      0.57       506\n",
            "   macro avg       0.49      0.43      0.43       506\n",
            "weighted avg       0.54      0.57      0.53       506\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VlMbhElFAjVG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2255
        },
        "outputId": "67a31413-afbc-4d86-dd21-63db5073e71a"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class TextStats(BaseEstimator, TransformerMixin):\n",
        "  \n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, tweet):\n",
        "        return dict_tweet(tweet)\n",
        "                \n",
        "\n",
        "\n",
        "pipeline =  Pipeline([\n",
        "\n",
        "    # Use FeatureUnion to combine the features from subject and body\n",
        "    ('union', FeatureUnion(\n",
        "        transformer_list=[\n",
        "\n",
        "            # Pipeline for pulling features from the post's subject line\n",
        "            ('ngram', Pipeline([\n",
        "                ('vect', TfidfVectorizer(min_df=1,\n",
        "                             max_df = 0.99,\n",
        "                             sublinear_tf=False,\n",
        "                             analyzer='word',\n",
        "                             ngram_range = (1,1),\n",
        "                             max_features = None,\n",
        "                             strip_accents = \"ascii\",\n",
        "                             lowercase = True,\n",
        "                             smooth_idf = True,\n",
        "                             use_idf=False)),\n",
        "                ('tfidf', TfidfTransformer(norm='l2', \n",
        "                                           use_idf=True,\n",
        "                                           smooth_idf=True,\n",
        "                                           sublinear_tf=False)),\n",
        "            ])),\n",
        "\n",
        "\n",
        "            # Pipeline for pulling ad hoc features from post's body\n",
        "            ('diccionari', Pipeline([\n",
        "                ('stats', TextStats()),  # returns a list of dicts\n",
        "                ('vect', DictVectorizer()),  # list of dicts -> feature matrix\n",
        "            ])),\n",
        "\n",
        "        ],\n",
        "\n",
        "        # weight components in FeatureUnion\n",
        "        transformer_weights={\n",
        "            'subjngramect': 1.0,\n",
        "            'diccionari': 0.0,\n",
        "        },\n",
        "    )),\n",
        "\n",
        "    # Use a SVC classifier on the combined features\n",
        "    ('clf', svm.LinearSVC(penalty='l2',\n",
        "                          loss='squared_hinge',\n",
        "                          dual=True,\n",
        "                          tol=0.0001, \n",
        "                          C=1.0, \n",
        "                          multi_class='ovr',\n",
        "                          fit_intercept=True,\n",
        "                          intercept_scaling=1,\n",
        "                          class_weight=None,\n",
        "                          verbose=0,\n",
        "                          random_state=None,\n",
        "                          max_iter=1000)),\n",
        "])\n",
        "\n",
        "\n",
        "#print (text_clf.named_steps['union'].transformer_list[0][1].named_steps['vect'].get_feature_names())\n",
        "\n",
        "\n",
        "pipeline.fit(trainCorpus, train_labels)\n",
        "\n",
        "predicted = pipeline.predict(devCorpus)\n",
        "\n",
        "\n",
        "print(classification_report(dev_labels, predicted))\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-f7c396c62dd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainCorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevCorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \"\"\"\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    228\u001b[0m                 Xt, fitted_transformer = fit_transform_one_cached(\n\u001b[1;32m    229\u001b[0m                     \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                     **fit_params_steps[name])\n\u001b[0m\u001b[1;32m    231\u001b[0m                 \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, **fit_params)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_fit_transform_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    791\u001b[0m             delayed(_fit_transform_one)(trans, X, y, weight,\n\u001b[1;32m    792\u001b[0m                                         **fit_params)\n\u001b[0;32m--> 793\u001b[0;31m             for name, trans, weight in self._iter())\n\u001b[0m\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, **fit_params)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_fit_transform_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlast_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlast_step\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mXt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/dict_vectorizer.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0mFeature\u001b[0m \u001b[0mvectors\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0malways\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \"\"\"\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfitting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/dict_vectorizer.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, X, fitting)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;31m# same time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"%s%s%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseparator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py\u001b[0m in \u001b[0;36miteritems\u001b[0;34m(d, **kw)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[0;34m\"\"\"Return an iterator over the (key, value) pairs of a dictionary.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_iteritems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0miterlists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'items'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "nmRg7MlPaRHK",
        "colab_type": "code",
        "outputId": "bb7799fe-978d-4d44-a2b9-f49a5bd86f16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1431
        }
      },
      "cell_type": "code",
      "source": [
        "# Perform classification with SVM, kernel=rbf\n",
        "    classifier_rbf = svm.SVC()\n",
        "    t0 = time.time()\n",
        "    classifier_rbf.fit(train_vectors, train_labels)\n",
        "    t1 = time.time()\n",
        "    prediction_rbf = classifier_rbf.predict(dev_vectors)\n",
        "    t2 = time.time()\n",
        "    time_rbf_train = t1-t0\n",
        "    time_rbf_predict = t2-t1\n",
        "\n",
        "    # Perform classification with SVM, kernel=linear\n",
        "    classifier_linear = svm.SVC(kernel='linear')\n",
        "    t0 = time.time()\n",
        "    classifier_linear.fit(train_vectors, train_labels)\n",
        "    t1 = time.time()\n",
        "    prediction_linear = classifier_linear.predict(dev_vectors)\n",
        "    t2 = time.time()\n",
        "    time_linear_train = t1-t0\n",
        "    time_linear_predict = t2-t1\n",
        "\n",
        "    # Perform classification with SVM, kernel=linear\n",
        "    classifier_liblinear = svm.LinearSVC()\n",
        "    t0 = time.time()\n",
        "    classifier_liblinear.fit(train_vectors, train_labels)\n",
        "    t1 = time.time()\n",
        "    prediction_liblinear = classifier_liblinear.predict(dev_vectors)\n",
        "    t2 = time.time()\n",
        "    time_liblinear_train = t1-t0\n",
        "    time_liblinear_predict = t2-t1\n",
        "\n",
        "    # Perform classification with GaussianNB\n",
        "    dense_train = train_vectors.toarray()\n",
        "    dense_dev = dev_vectors.toarray()\n",
        "    classifier_nv = GaussianNB()\n",
        "    t0 = time.time()\n",
        "    classifier_nv.fit(dense_train, train_labels)\n",
        "    t1 = time.time()\n",
        "    prediction_classifier_nv = classifier_nv.predict(dense_dev)\n",
        "    t2 = time.time()\n",
        "    time_gnv_train = t1-t0\n",
        "    time_gnv_predict = t2-t1\n",
        "    \n",
        "    # Perform classification with MultinomialNB\n",
        "    dense_train = train_vectors.toarray()\n",
        "    dense_dev = dev_vectors.toarray()\n",
        "    classifier_mnv = MultinomialNB()\n",
        "    t0 = time.time()\n",
        "    classifier_mnv.fit(dense_train, train_labels)\n",
        "    t1 = time.time()\n",
        "    prediction_classifier_mnv = classifier_mnv.predict(dense_dev)\n",
        "    t2 = time.time()\n",
        "    time_mnv_train = t1-t0\n",
        "    time_mnv_predict = t2-t1\n",
        "    \n",
        "    \n",
        "    # Perform classification with BernoulliNB\n",
        "    dense_train = train_vectors.toarray()\n",
        "    dense_dev = dev_vectors.toarray()\n",
        "    classifier_bnv = BernoulliNB()\n",
        "    t0 = time.time()\n",
        "    classifier_bnv.fit(dense_train, train_labels)\n",
        "    t1 = time.time()\n",
        "    prediction_classifier_bnv = classifier_bnv.predict(dense_dev)\n",
        "    t2 = time.time()\n",
        "    time_bnv_train = t1-t0\n",
        "    time_bnv_predict = t2-t1\n",
        "    \n",
        "  \n",
        "    # Print results in a nice table\n",
        "    print(\"Results for SVC(kernel=rbf)\")\n",
        "    print(\"Training time: %fs; Prediction time: %fs\" % (time_rbf_train, time_rbf_predict))\n",
        "    print(classification_report(dev_labels, prediction_rbf))\n",
        "    \n",
        "    print(\"Results for SVC(kernel=linear)\")\n",
        "    print(\"Training time: %fs; Prediction time: %fs\" % (time_linear_train, time_linear_predict))\n",
        "    print(classification_report(dev_labels, prediction_linear))\n",
        "    \n",
        "    print(\"Results for LinearSVC()\")\n",
        "    print(\"Training time: %fs; Prediction time: %fs\" % (time_liblinear_train, time_liblinear_predict))\n",
        "    print(classification_report(dev_labels, prediction_liblinear))\n",
        "    \n",
        "    print(\"Results for GaussianNB()\")\n",
        "    print(\"Training time: %fs; Prediction time: %fs\" % (time_gnv_train, time_gnv_predict))\n",
        "    print(classification_report(dev_labels, prediction_classifier_nv))\n",
        "    \n",
        "    print(\"Results for MultinomialNB()\")\n",
        "    print(\"Training time: %fs; Prediction time: %fs\" % (time_mnv_train, time_mnv_predict))\n",
        "    print(classification_report(dev_labels, prediction_classifier_mnv))\n",
        "    \n",
        "    print(\"Results for BernoulliNB()\")\n",
        "    print(\"Training time: %fs; Prediction time: %fs\" % (time_bnv_train, time_bnv_predict))\n",
        "    print(classification_report(dev_labels, prediction_classifier_bnv))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Results for SVC(kernel=rbf)\n",
            "Training time: 0.191416s; Prediction time: 0.069244s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           N       0.43      1.00      0.60       219\n",
            "         NEU       0.00      0.00      0.00        69\n",
            "        NONE       0.00      0.00      0.00        62\n",
            "           P       0.00      0.00      0.00       156\n",
            "\n",
            "   micro avg       0.43      0.43      0.43       506\n",
            "   macro avg       0.11      0.25      0.15       506\n",
            "weighted avg       0.19      0.43      0.26       506\n",
            "\n",
            "Results for SVC(kernel=linear)\n",
            "Training time: 0.260540s; Prediction time: 0.064028s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           N       0.55      0.87      0.67       219\n",
            "         NEU       0.50      0.04      0.08        69\n",
            "        NONE       0.00      0.00      0.00        62\n",
            "           P       0.57      0.54      0.56       156\n",
            "\n",
            "   micro avg       0.55      0.55      0.55       506\n",
            "   macro avg       0.40      0.36      0.33       506\n",
            "weighted avg       0.48      0.55      0.47       506\n",
            "\n",
            "Results for LinearSVC()\n",
            "Training time: 0.017342s; Prediction time: 0.000339s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           N       0.59      0.82      0.69       219\n",
            "         NEU       0.33      0.12      0.17        69\n",
            "        NONE       0.42      0.18      0.25        62\n",
            "           P       0.60      0.59      0.59       156\n",
            "\n",
            "   micro avg       0.57      0.57      0.57       506\n",
            "   macro avg       0.49      0.43      0.43       506\n",
            "weighted avg       0.54      0.57      0.53       506\n",
            "\n",
            "Results for GaussianNB()\n",
            "Training time: 0.090842s; Prediction time: 0.079241s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           N       0.56      0.50      0.53       219\n",
            "         NEU       0.15      0.20      0.17        69\n",
            "        NONE       0.22      0.15      0.17        62\n",
            "           P       0.42      0.47      0.44       156\n",
            "\n",
            "   micro avg       0.41      0.41      0.41       506\n",
            "   macro avg       0.34      0.33      0.33       506\n",
            "weighted avg       0.42      0.41      0.41       506\n",
            "\n",
            "Results for MultinomialNB()\n",
            "Training time: 0.028278s; Prediction time: 0.007605s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           N       0.48      0.99      0.65       219\n",
            "         NEU       0.00      0.00      0.00        69\n",
            "        NONE       0.00      0.00      0.00        62\n",
            "           P       0.81      0.31      0.45       156\n",
            "\n",
            "   micro avg       0.52      0.52      0.52       506\n",
            "   macro avg       0.32      0.32      0.27       506\n",
            "weighted avg       0.46      0.52      0.42       506\n",
            "\n",
            "Results for BernoulliNB()\n",
            "Training time: 0.064112s; Prediction time: 0.029447s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           N       0.48      0.99      0.64       219\n",
            "         NEU       0.00      0.00      0.00        69\n",
            "        NONE       0.00      0.00      0.00        62\n",
            "           P       0.85      0.28      0.42       156\n",
            "\n",
            "   micro avg       0.52      0.52      0.52       506\n",
            "   macro avg       0.33      0.32      0.27       506\n",
            "weighted avg       0.47      0.52      0.41       506\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}